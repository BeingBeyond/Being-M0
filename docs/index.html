<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Scaling Motion Generation Model with  Million-Level Motion Benchmark">
    <meta name="keywords" content="Humanoid Robot, Agent, VLM, Dexterous Manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Scaling Motion Generation Model with  Million-Level Motion Benchmark</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=??"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', '???');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./css/bulma.min.css">
    <link rel="stylesheet" href="./css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./css/bulma-slider.min.css">
    <link rel="stylesheet" href="./css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./css/index.css">
    <link rel="icon" href="./images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>
</head>
<body>


    <section class="hero is-link is-fullheight video" style="overflow: hidden; position:relative;">
        <div class="hero-video" style="height: 100%; width: 177.77777778vh; min-width: 100%;min-height: 56.25vw;">
          <img src="./images/dataset_alt.png" alt="Dataset Image" style="width: 100%; height: 100%; object-fit: cover;">
        </div>
        <div class="hero-video is-hidden-tablet is-inline-block-mobile"
          style="height: 154.28571428vw; width: 100%; min-width:64.81481481vh;min-height:100%;">
          <img src="./images/dataset_alt.png" alt="Dataset Image" style="width: 100%; height: 100%; object-fit: cover;">
        </div>
        <div class="overlay"></div>
        <!-- Hero head: will stick at the top -->
        <div class="hero-head is-hidden-mobile">
          <header class="navbar">
            <div class="container is-size-5">
              <div class="navbar-menu">
                <div class="navbar-end">
                  <a class="navbar-item pl-4 pr-4" href="https://arxiv.org/pdf/2410.03311">
                    <span class="icon" style="margin-right:5px;">
                      <img src="./images/icon/pdf.svg" alt="PDF" />
                    </span>
                    <span>Paper</span></a>
                  <a class="navbar-item  pl-4 pr-4" href="https://arxiv.org/abs/2410.03311">
                    <span class="icon" style="margin-right:5px;">
                      <img src="./images/icon/arxiv.svg" alt="arXiv" />
                    </span>
                    <span>arXiv</span></a>
                  <a class="navbar-item  pl-4 pr-4" href="https://github.com/BeingBeyond/Being-M-0" >
                    <span class="icon" style="margin-right:5px;">
                      <img src="./images/icon/github.svg" alt="github" />
                    </span>
                    <span>Project</span></a>
                </div>
              </div>
            </div>
          </header>
        </div>
      
        <!-- Hero content: will be in the middle -->
        <!-- rgb(211, 151, 21)-->>
        <div class="hero-body">
          <div class="container has-text-centered">
            <h1 class="title is-1 publication-title is-size-1-mobile" style="font-size: 10rem;">
                <span style="font-weight: bold;color:cornflowerblue;">Being-M-0</span>
            </h1>
            <h1 class="subtitle is-1 publication-title is-size-4-mobile" style="font-size: 4rem;">
              Scaling Motion Generation Model <br class="is-hidden-mobile"> with Million-Level Motion Benchmark
            </h1>
            <h1 class="subtitle is-1 publication-title is-size-5-mobile" style="font-size: 2rem;">
              <br>RUC &nbsp;&nbsp; BAAI &nbsp;&nbsp; PKU &nbsp;&nbsp; BeingBeyond
            </h1>
            <div class="column has-text-centered is-hidden-tablet">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.03311" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="./images/icon/pdf.svg" alt="PDF" />
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.03311" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="./images/icon/arxiv.svg" alt="arXiv" />
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/BeingBeyond/Being-M-0"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="./images/icon/github.svg" alt="Github" />
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      
        <!-- Hero footer: will stick at the bottom -->
        <!--
        
        <div class="hero-foot is-hidden-mobile">
          <nav class="tabs is-boxed is-fullwidth is-size-5">
            <ul>
              <li><a href="#umi">Robot Data without Robots</a></li>
              <li><a href="#task-tracking">Task Tracking without Simulating Tasks</a></li>
              <li><a href="#wbc">Whole-body Controller</a></li>
              <li><a href="#hardware">Hardware & System</a></li>
              <li><a href="#qa">Q & A</a></li>
            </ul>
          </nav>
        </div>
        -->
    </section>

<section class="hero">
    <div class="hero-body">
        <div class="container">
            <div class="columns is-centered">
                <div class="column has-text-centered">

                    <h1 class="title is-1 publication-title is-size-4-mobile">Scaling Motion Generation Model with  Million-level Motion Benchmark</h1>
                    <h1> 
                    <div class="is-size-4 publication-authors is-size-6-mobile">
                    <span class="author-block">
                        <a href="">Ye Wang</a>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                        <a href="">Sipeng Zheng</a>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                        <a href="">Bin Cao</a>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                        <a href="">Qianshan Wei</a>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                      <a href="">Weishuai Zeng</a>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                        <a href="">Qin Jin</a>&nbsp;&nbsp;
                    </span>
                    <span class="author-block">
                      <a href="https://z0ngqing.github.io/">Zongqing Lu</a><sup>&sect;</sup>
                    </span>
                    </div>
                    </h1>
                   
                    <br>
                    <div class="is-size-4 publication-authors is-size-6-mobile">
                        <span class="author-block" style="font-size: 100%"> RUC </span>&nbsp;&nbsp;&nbsp;
                        <span class="author-block" style="font-size: 100%"> BAAI </span>&nbsp;&nbsp;&nbsp;
                        <span class="author-block" style="font-size: 100%"> PKU </span>&nbsp;&nbsp;&nbsp;
                        <span class="author-block" style="font-size: 100%"> BeingBeyond </span>
                    </div>

                    <div class="is-size-5 publication-authors is-size-7-mobile">
                        <span class="author-block"><sup>&sect;</sup> Corresponding author</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.03311"
                        class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2410.03311"
                        class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Video Links -->
                            <!--              <span class="link-block">-->
                            <!--                <a href="https://www.youtube.com/watch?v=Cx-D708BedY"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="fab fa-youtube"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Video - Main Storyline</span>-->
                            <!--                </a>-->
                            <!--              </span>-->
                            <!--              <span class="link-block">-->
                            <!--                <a href="https://www.youtube.com/watch?v=Oa4Ese8mMD0"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="fab fa-youtube"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Video - Open-ended World</span>-->
                            <!--                </a>-->
                            <!--              </span>-->
                            <!-- Code Link. -->
                            <span class="link-block">
                <a href="https://github.com/BeingBeyond/Being-M-0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Project</span>
                  </a>
              </span>
                        </div>
                    </div>

                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
      <div class="container">
          <div class="three-item">
            <div class="item">
                <img src="./images/bicep_curl_exercise_video.gif" height="100%">
            </div>
            <div class="item">
                <img src="./images/downtown_walking_video.gif" height="100%">
            </div>
            <div class="item">
                <img src="./images/talking_video.gif" height="100%">
            </div>
          <div>
      </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
      <div class="container">
          <div class="three-item">
            <div class="item">
                <img src="./images/bicep_curl_exercise.gif" height="100%">
            </div>
            <div class="item">
                <img src="./images/downtown_walking.gif" height="100%">
            </div>
            <div class="item">
                <img src="./images/talking.gif" height="100%">
            </div>
          <div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <h2 class="title is-3 is-size-4-mobile">Abstract</h2>
              <div class="content has-text-justified">
                  <p>
                    Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models.
                    Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. 
                    To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15&times; larger than existing counterparts and enriched with hierarchical text descriptions.
                    Using MotionLib, we train a large motion model named <span class="method" style="font-weight: bold; color:cornflowerblue;">Being-M-0</span>, demonstrating robust performance across a wide range of human activities, including unseen ones.
                    Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal.
                    To better integrate the motion modality, we propose MotionBook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens.
                    We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future.
                  </p>
              </div>
          </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
      <div class="column one-three-thirds">
          <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Being-M-0</h2>
          <img src="./images/motion.png" height="100%">
          <br><br>
          <h2>
              <div class="content has-text-justified">
              <span style="font-weight: bold;">[Overview]</span> This work introduce, <span style="font-weight: bold;color:cornflowerblue;">MotionBase-0</span>, 
                    which comprises three key components: 
                  <ul>
                      <li> <b>Dataset: </b> the first large-scale motion generation benchmark containing over one million motions with detailed
                        textual descriptions, significantly advancing the capability to effectively train motion generation
                        models;</li>
                      <li> <b>Key Insights: </b> Our research identifies critical factors affecting the effectiveness of large
                        motion models, emphasizing the importance of scaling both data and model size. Additionally, we
                        uncover limitations in the current evaluation metrics, particularly when handling diverse and unseen
                        motions;</li>
                      <li> <b>Novel Motion Quantization</b> We propose a novel motion quantization approach that
                        represents motion clips as 2D images and constructs a finite-scale codebook without requiring token
                        lookups. This method retains essential information and expands the capacity of the motion encoder,
                        enhancing the ability of large motion models to leverage large-scale motion data.</li>
                  </ul>
     
              </div> 
          </h2>
          <br><br><br>
          <img src="./images/model.png">
          <br><br>
          <h2>
              <div class="content has-text-justified">
              <span style="font-weight: bold;">[Scaling Law of Motion Generation]</span> 
              In this paper, we investigate whether large motion models can serve as a promising direction for motion generation.
              Our key finding is that scaling both data and model size substantially reduces joint prediction errors on critical metrics while enhancing generalization to novel motions. 
              Beyond these factors, we identify inadequate motion representation as a fundamental constraint on large motion models. 
              To overcome this limitation, we propose representing motion clips as single-channel 2D images and introduce 2D-LFQ, a novel motion quantization method.
              </div>
          </h2>
      </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
          <div class="column is-three-thirds">
              <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Sim and Real Application</h2>
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel1" class="carousel results-carousel">
                    <div class="item item-a">
                      <img id="a" src="./images/sim_1.gif"  height="100%">
                    </div>
                    <div class="item item-b">
                      <img id="a" src="./images/sim_2.gif"  height="100%">
                    </div>
                    <div class="item item-c">
                      <img id="a" src="./images/sim_3.gif"  height="100%">
                    </div>
                    <div class="item item-d">
                      <img id="a" src="./images/sim_4.gif"  height="100%">
                    </div>
                    <div class="item item-e">
                      <img id="a" src="./images/sim_5.gif"  height="100%">
                    </div>
                    <div class="item item-e">
                      <img id="a" src="./images/sim_6.gif"  height="100%">
                    </div>
                  </div>
                </div>
              </div>
              <h2>
                  <div class="content has-text-justified">
                      <span style="font-weight: bold;">[Animation Motion Tracking]</span>  
                      Our model allows motion retargeting to any animated character in the simulated world while adhering to physical laws.
                  </div>
              </h2>    
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel1" class="carousel results-carousel">
                    <div class="item item-a">
                      <img id="a" src="./images/robot_1.gif" height="100%">
                    </div>
                    <div class="item item-b">
                      <img id="b" src="./images/robot_2.gif" height="100%">
                    </div>
                    <div class="item item-c">
                      <img id="c" src="./images/robot_3.gif" height="100%">
                    </div>
                    <div class="item item-d">
                      <img id="d" src="./images/robot_4.gif" height="100%">
                    </div>
                    <div class="item item-e">
                      <img id="e" src="./images/robot_5.gif" height="100%">
                    </div>
                    <div class="item item-e">
                      <img id="e" src="./images/robot_6.gif" height="100%">
                    </div>
                  </div>
                </div>
              </div>
              <h2>
                <div class="content has-text-justified">
                    <span style="font-weight: bold;">[Robot Motion Tracking]</span>  
                    More importantly, the generated motions can be retargeted to robots like H1, enabling them to perform human-like actions without pre-defined execution.
                    In this work, we demonstrate simple robot demos (e.g., hanging in the air). 
                    For more advanced motion tracking—where the robot follows human motions while adhering to physical laws (e.g., maintaining balance) --
                    our latest project <a href="https://github.com/BeingBeyond/Jaeger/blob/main/xxx">Jaeger</a>.
                </div>
            </h2>   
            </section>
          </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <h2 class="title is-3 is-size-4-mobile"></h2>
              <div class="content has-text-justified">
                  <p>
                       
                  </p>
              </div>
          </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
          <div class="column is-three-thirds">
              <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Dataset</h2>
              <img src="./images/dataset_static.png" alt="vs fixedcam" style="width: 100%; height: auto;">
              <br><br>
              <h2>
                  <div class="content has-text-justified">
                      <span style="font-weight: bold;">[Comparison with existing human motion datasets]</span>  In the table, B, H, and F refer to body, hand, and face, respectively. “part” indicates that the
                      text captions include fine-grained descriptions of body parts, while “body” means the descriptions
                      are not as detailed. “multi” and “single” specify whether the dataset contains multi-person scenarios
                      or only single-person data. Our MotionBase is the largest motion generation dataset and benchmark
                      to date, featuring at least 15× more data than previous datasets, along with additional modalities.
                  </div>
              </h2>  
              <br><br>  
              <img src="./images/dataset_distribute.png" alt="vs fixedcam" style="width: 100%; height: auto;">
              <h2>
                <div class="content has-text-justified">
                    <span style="font-weight: bold;">[Distribution of MotionBase]</span>  
                    We present the scale (LEFT) and length (RIGHT) Distribution of motion sequences across subsets of MotionBase. 
                    To be specific, MotionBase contains over 1 million motion sequences from 42 different public datasets and web
                    videos on the Internet. Subsets of MotionX, including Animation, Perform, Dance, Aist, Kungfu, GRAB, 
                    Music, Idea400, HAA500, Game Motion, and Fitness, are included in MotionBase. 
                    Recognizing the high cost of collecting and annotating videos, we also see the untapped potential of images for motion understanding. 
                    Consequently, MotionBase incorporates image data by repeating each image across 64 frames and treating it as a motion sequence.
                    For the datasets with long-range videos, such as MPI-INF-3DHP, we segment the footage into sub-clips with 
                    random durations ranging from 10 seconds to one minute.
                </div>
            </h2>  
          </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
              <h2 class="title is-3 is-size-4-mobile"></h2>
              <div class="content has-text-justified">
                  <p>
                       
                  </p>
              </div>
          </div>
      </div>
  </div>
</section>


  




<section class="section">
    <div class="container">
        <div class="columns is-centered has-text-centered">
            <div class="column is-three-thirds">
                <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Experiments</h2>
                <img src="./images/exp_scaling.png" alt="vs wo-connector" style="width: 100%; height: auto;">
                <br><br>
                <h2>
                    <div class="content has-text-justified">
                        <span style="font-weight: bold;">[Scaling of model size and dataset benefits motion generation]</span> 
                        This table shows the comparisons under different model and data sizes. All experiments are conducted using
                        the same pretrained VQ model for consistency. Additionally, we re-train the motion autoencoder
                        and text encoder separately on the Motion-X and MotionBase datasets.
                        Our results demonstrate that increasing both model size and data amount leads to significant performance improvements.
                    </div>
                </h2>
                <br><br><br>
                <img src="./images/exp_comparison.png" alt="vs wo-adjust" style="width: 100%; height: auto;">
                <br><br>
                <h2>
                    <div class="content has-text-justified">
                        <span style="font-weight: bold;">[Comparison with SoTA]</span> 
                        We evaluate our large motion model on the widely adopted HumanML3D benchmark.
                        We compare its performance against a variety of SoTA approaches. Our model, which utilizes Llama-2-13B as
                        the decoder and calculates the loss over the entire concatenated sequence of input text, achieves SOTA performance. 
                        Our large motion model significantly outperforms other LLM-based methods such as MotionGPT and AvatarGPT, as well as the earlier T2MGPT. 
                       
                    </div>
                    <div class="content has-text-justified">
                      <span style="font-weight: bold;">[Out-of-Domain evaluation]</span> 
                      This ablation is essential for further validating the generalization capabilities of large motion models, 
                      as the improvements may stem from the inclusion of additional in-domain data from Motion-X. 
                      In this setup, we select four subsets from MotionBase, comprising 90K samples (UNSEEN-90K), for evaluation, while the remaining 38 subsets are used for training.
                  </div>
                    
                </h2>    
                <br><br><br>
                <img src="./images/exp_vq.png" alt="vs fixedcam" style="width: 100%; height: auto;">
                <br><br>
                <h2>
                    <div class="content has-text-justified">
                        <span style="font-weight: bold;">[FID results on different motion quantization]</span> 
                        We compare our proposed 2D lookup-free quantization (2D-LFQ) against two commonly used approaches: residual vector quantization (RVQ) and vector quantization (VQ).
                        2D-LFQ demonstrates significant improvements over both RVQ and VQ. 
                        Notably, as the codebook size increases, 2D-LFQ continues to enhance performance, 
                        while RVQ and VQ experience diminishing returns or performance degradation with larger codebooks.
                    </div>
                </h2>    
            </div>
        </div>
    </div>
</section>
                
<section class="section">
    <div class="container">
        <div class="columns is-centered has-text-centered">
            <div class="column is-two-thirds">
                <h2 class="title is-3 is-size-4-mobile" style="text-align: center;">Conclusion</h2>
                <div class="content has-text-justified">
                    <p>
                      In this paper, we explore how to advance the field of large-scale motion generation. To this end, we
                      introduce a large-scale motion dataset named MotionBase, which includes detailed text descriptions
                      and rich modality annotations, providing a strong foundation for effectively training large motion
                      models. Our research highlights key findings, such as the impact of scaling both data and model size.
                      Additionally, we identify potential limitations in the current evaluation metrics, particularly when
                      assessing diverse and unseen motions. To enhances the benefits large motion models can derive from
                      extensive motion data, we propose a novel motion quantization approach that treats motion clips as
                      2D images and constructs a finite-scale codebook, eliminating the need for token lookups. We hope
                      that this research offers valuable direction for future work in large-scale motion generation.
                    </p>
  
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container content">
        <h2 style="margin-left: 14%">BibTeX</h2>
        <pre><code>@article{yuan2025being,
title={Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models},
author={Wang, Ye and Zheng, Sipeng and Cao, Bin and Wei, Qianshan and Jin, Qin and Lu, Zongqing},
journal={arXiv preprint arXiv:2410.03311},
year={2025}
}</code></pre>
    </div>
</section>




<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website template is licensed under a <a rel="license"
                                                                     href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a> and adapted from source at <a
                            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://umi-on-legs.github.io/">UMI</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>